{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidP0011/apps_functions/blob/main/app_transc_01_files_path_collect.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxXmQcqu9E_1"
      },
      "source": [
        "# INICIALIZACIÓN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EPbRzlCq9Clc",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "def install_dpm_repos(config: dict) -> None:\n",
        "    \"\"\"\n",
        "    Instala repositorios DPM y lista las funciones de cada paquete instalado.\n",
        "\n",
        "    El diccionario de configuración debe tener la clave:\n",
        "        - github_repo_url_list: Lista de URLs de repositorios GitHub\n",
        "          Ejemplo:\n",
        "            {\n",
        "                \"github_repo_url_list\": [\n",
        "                  \"https://github.com/DavidP0011/common_functions\",\n",
        "                  \"https://github.com/DavidP0011/utils_functions\",\n",
        "                  \"https://github.com/DavidP0011/etl_functions\"\n",
        "                  ]\n",
        "            }\n",
        "\n",
        "    La función procede en dos pasos para cada repositorio:\n",
        "      1. Se instala de manera “normal” (pip install --upgrade --no-cache-dir git+<repo_url>)\n",
        "         para instalar las dependencias si faltan.\n",
        "      2. Se fuerza la reinstalación de los archivos .py con --force-reinstall y --no-deps\n",
        "         (pip install --upgrade --force-reinstall --no-deps --no-cache-dir git+<repo_url>).\n",
        "\n",
        "    Luego, se detecta dinámicamente el nombre del paquete instalado y se recorre\n",
        "    cada paquete para imprimir las funciones definidas.\n",
        "    \"\"\"\n",
        "    import subprocess\n",
        "    import sys\n",
        "    import importlib\n",
        "    import pkgutil\n",
        "    import inspect\n",
        "\n",
        "    # Para detectar distribuciones y top-level packages\n",
        "    try:\n",
        "        import importlib.metadata as metadata\n",
        "    except ImportError:\n",
        "        # Si la versión de Python es muy antigua:\n",
        "        import importlib_metadata as metadata\n",
        "\n",
        "    def _validate_config(cfg: dict) -> list:\n",
        "        \"\"\"Valida que la configuración tenga la clave 'github_repo_url_list' con una lista.\"\"\"\n",
        "        if \"github_repo_url_list\" not in cfg:\n",
        "            raise ValueError(\"[VALIDATION [ERROR ❌]] La configuración debe contener la clave 'github_repo_url_list'.\")\n",
        "        if not isinstance(cfg[\"github_repo_url_list\"], list):\n",
        "            raise ValueError(\"[VALIDATION [ERROR ❌]] La clave 'github_repo_url_list' debe ser una lista de URLs.\")\n",
        "        return cfg[\"github_repo_url_list\"]\n",
        "\n",
        "    def _install_repo(repo_url: str) -> None:\n",
        "        \"\"\"\n",
        "        Instala un repositorio en dos pasos:\n",
        "          1. Instala el repositorio de forma normal (para que se instalen las dependencias si aún no están).\n",
        "          2. Fuerza la reinstalación del código (los archivos .py) sin tocar las dependencias (--no-deps).\n",
        "        \"\"\"\n",
        "        print(f\"\\n[START ▶️] Instalación inicial (con dependencias) del repositorio: {repo_url}\", flush=True)\n",
        "        cmd_normal = [\n",
        "            sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n",
        "            \"--upgrade\", \"--no-cache-dir\",\n",
        "            f\"git+{repo_url}\"\n",
        "        ]\n",
        "        try:\n",
        "            subprocess.check_call(cmd_normal)\n",
        "            print(f\"[SUCCESS ✅] Instalación inicial completada.\", flush=True)\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"[ERROR ❌] Falló la instalación inicial del repositorio. Detalle: {e}\", flush=True)\n",
        "\n",
        "        print(f\"\\n[START ▶️] Forzando reinstalación del código del repositorio: {repo_url}\", flush=True)\n",
        "        cmd_force = [\n",
        "            sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n",
        "            \"--upgrade\", \"--force-reinstall\", \"--no-deps\", \"--no-cache-dir\",\n",
        "            f\"git+{repo_url}\"\n",
        "        ]\n",
        "        try:\n",
        "            subprocess.check_call(cmd_force)\n",
        "            print(f\"[SUCCESS ✅] Reinstalación del código completada.\", flush=True)\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"[ERROR ❌] Falló la reinstalación del código. Detalle: {e}\", flush=True)\n",
        "\n",
        "    def _detect_installed_package_name(before_set: set, repo_url: str) -> str:\n",
        "        \"\"\"\n",
        "        Compara las distribuciones antes y después de la instalación:\n",
        "        - before_set: conjunto de nombres de distribución (metadata[\"Name\"]) previos.\n",
        "        - repo_url: URL del repositorio (para intentar emparejar por substring).\n",
        "        Devuelve el nombre de paquete (top-level) para importar.\n",
        "        \"\"\"\n",
        "        # Obtenemos el conjunto actual de distribuciones instaladas\n",
        "        after_set = {dist.metadata[\"Name\"] for dist in metadata.distributions()}\n",
        "\n",
        "        # Identificamos cuáles son las nuevas distribuciones\n",
        "        nuevas = list(after_set - before_set)\n",
        "\n",
        "        repo_name = repo_url.rstrip(\"/\").split(\"/\")[-1].lower()\n",
        "\n",
        "        if not nuevas:\n",
        "            # Si no hay distribuciones nuevas, intentamos emparejar alguna existente por substring\n",
        "            candidatos = [dist.metadata[\"Name\"] for dist in metadata.distributions() if repo_name in dist.metadata[\"Name\"].lower()]\n",
        "            if candidatos:\n",
        "                elegida_dist = candidatos[0]\n",
        "                print(f\"[INFO ℹ️] No se detectó distribución nueva. Se asume '{elegida_dist}' para '{repo_name}'.\", flush=True)\n",
        "            else:\n",
        "                print(f\"[WARN ⚠️] No se detectó nueva distribución ni coincidencias para '{repo_name}'. Se usará '{repo_name}' por defecto.\", flush=True)\n",
        "                return repo_name\n",
        "        else:\n",
        "            # Si hay más de una nueva, intentamos filtrar por substring\n",
        "            if len(nuevas) > 1:\n",
        "                filtradas = [d for d in nuevas if repo_name in d.lower()]\n",
        "                elegida_dist = filtradas[0] if filtradas else nuevas[0]\n",
        "                print(f\"[INFO ℹ️] Se detectaron múltiples distribuciones nuevas {nuevas}. Se elige '{elegida_dist}'.\", flush=True)\n",
        "            else:\n",
        "                elegida_dist = nuevas[0]\n",
        "            print(f\"[INFO ℹ️] Se detectó la nueva distribución '{elegida_dist}'.\", flush=True)\n",
        "\n",
        "        # Ahora obtenemos el top-level package desde la metadata\n",
        "        try:\n",
        "            dist_info = metadata.distribution(elegida_dist)\n",
        "            top_level_txt = dist_info.read_text(\"top_level.txt\")\n",
        "            if top_level_txt:\n",
        "                top_levels = [line.strip() for line in top_level_txt.splitlines() if line.strip()]\n",
        "                paquete = top_levels[0] if top_levels else elegida_dist\n",
        "                print(f\"[INFO ℹ️] Para la distribución '{elegida_dist}', se detectó el paquete top-level: '{paquete}'\", flush=True)\n",
        "                return paquete\n",
        "            else:\n",
        "                print(f\"[WARN ⚠️] 'top_level.txt' vacío para '{elegida_dist}'. Se usará '{elegida_dist}' como paquete.\", flush=True)\n",
        "                return elegida_dist\n",
        "        except Exception as e:\n",
        "            # Si no existe top_level.txt o falla, devolvemos el nombre de la distribución\n",
        "            print(f\"[ERROR ❌] No se pudo leer 'top_level.txt' de '{elegida_dist}': {e}. Usando '{elegida_dist}'.\", flush=True)\n",
        "            return elegida_dist\n",
        "\n",
        "    def _list_functions(package) -> None:\n",
        "        \"\"\"\n",
        "        Recorre todos los módulos y submódulos del paquete e imprime las funciones definidas en cada uno.\n",
        "        \"\"\"\n",
        "        print(f\"\\n[START ▶️] Funciones en el paquete: {package.__name__}\", flush=True)\n",
        "        for finder, module_name, is_pkg in pkgutil.walk_packages(package.__path__, package.__name__ + \".\"):\n",
        "            try:\n",
        "                modulo = importlib.import_module(module_name)\n",
        "                funciones = [\n",
        "                    nombre for nombre, objeto in inspect.getmembers(modulo, inspect.isfunction)\n",
        "                    if inspect.getmodule(objeto) == modulo\n",
        "                ]\n",
        "                print(f\"\\n Módulo: {module_name}\", flush=True)\n",
        "                if funciones:\n",
        "                    for funcion in funciones:\n",
        "                        print(\"  -\", funcion, flush=True)\n",
        "                else:\n",
        "                    print(\"  (No se encontraron funciones definidas)\", flush=True)\n",
        "            except Exception as e:\n",
        "                print(f\"[ERROR ❌] Falló al importar el módulo {module_name}: {e}\", flush=True)\n",
        "\n",
        "    # Mensaje de inicio del proceso global\n",
        "    print(\"🔹🔹🔹 Instalación de Repositorios DPM y listado de funciones 🔹🔹🔹\", flush=True)\n",
        "\n",
        "    # Validación de la configuración\n",
        "    github_repo_url_list = _validate_config(config)\n",
        "\n",
        "    for repo_url in github_repo_url_list:\n",
        "        # Tomamos snapshot de distribuciones antes de instalar\n",
        "        antes = {dist.metadata[\"Name\"] for dist in metadata.distributions()}\n",
        "\n",
        "        # Instalamos el repositorio en dos pasos\n",
        "        _install_repo(repo_url)\n",
        "\n",
        "        # Detectamos automáticamente el nombre del paquete instalado\n",
        "        package_name = _detect_installed_package_name(antes, repo_url)\n",
        "\n",
        "        # Importamos el paquete y listamos sus funciones\n",
        "        try:\n",
        "            print(f\"\\n\\n🔹🔹🔹 Importando el paquete: {package_name} 🔹🔹🔹\", flush=True)\n",
        "            if package_name in sys.modules:\n",
        "                del sys.modules[package_name]\n",
        "            importlib.invalidate_caches()\n",
        "            paquete = importlib.import_module(package_name)\n",
        "            _list_functions(paquete)\n",
        "        except Exception as e:\n",
        "            print(f\"❌ [ERROR] Falló al importar el paquete '{package_name}': {e}\", flush=True)\n",
        "\n",
        "    print(\"\\n\\n🔹🔹🔹 [FINISHED ✅] Proceso completado. 🔹🔹🔹\", flush=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_UyYEYp9M2a",
        "outputId": "c59a3121-bf4c-4a63-9a10-5e7e50df5558"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔹🔹🔹 Instalación de Repositorios DPM y listado de funciones 🔹🔹🔹\n",
            "\n",
            "[START ▶️] Instalación inicial (con dependencias) del repositorio: https://github.com/DavidP0011/common_functions\n",
            "[SUCCESS ✅] Instalación inicial completada.\n",
            "\n",
            "[START ▶️] Forzando reinstalación del código del repositorio: https://github.com/DavidP0011/common_functions\n",
            "[SUCCESS ✅] Reinstalación del código completada.\n",
            "[INFO ℹ️] No se detectó distribución nueva. Se asume 'dpm_common_functions' para 'common_functions'.\n",
            "[INFO ℹ️] Para la distribución 'dpm_common_functions', se detectó el paquete top-level: 'dpm_common_functions'\n",
            "\n",
            "\n",
            "🔹🔹🔹 Importando el paquete: dpm_common_functions 🔹🔹🔹\n",
            "\n",
            "[START ▶️] Funciones en el paquete: dpm_common_functions\n",
            "\n",
            " Módulo: dpm_common_functions.dpm_GCP_ini_utils\n",
            "  - _ini_authenticate_API\n",
            "  - ini_environment_identification\n",
            "  - ini_google_drive_instalation\n",
            "\n",
            "[START ▶️] Instalación inicial (con dependencias) del repositorio: https://github.com/DavidP0011/utils_functions\n",
            "[SUCCESS ✅] Instalación inicial completada.\n",
            "\n",
            "[START ▶️] Forzando reinstalación del código del repositorio: https://github.com/DavidP0011/utils_functions\n",
            "[SUCCESS ✅] Reinstalación del código completada.\n",
            "[INFO ℹ️] No se detectó distribución nueva. Se asume 'dpm_utils_functions' para 'utils_functions'.\n",
            "[INFO ℹ️] Para la distribución 'dpm_utils_functions', se detectó el paquete top-level: 'dpm_utils_functions'\n",
            "\n",
            "\n",
            "🔹🔹🔹 Importando el paquete: dpm_utils_functions 🔹🔹🔹\n",
            "\n",
            "[START ▶️] Funciones en el paquete: dpm_utils_functions\n",
            "\n",
            " Módulo: dpm_utils_functions.dpm_old\n",
            "  - ini_install_libraries\n",
            "  - ini_load_dpm_libs\n",
            "\n",
            " Módulo: dpm_utils_functions.dpm_pdf_utils\n",
            "  - pdf_merge_intercalated_pages_file\n",
            "\n",
            " Módulo: dpm_utils_functions.dpm_tables\n",
            "  - DType_df_to_df\n",
            "  - fields_name_format\n",
            "  - table_DF_to_various_targets\n",
            "  - table_various_sources_to_DF\n",
            "  - tables_consolidate_duplicates_df\n",
            "\n",
            "[START ▶️] Instalación inicial (con dependencias) del repositorio: https://github.com/DavidP0011/apps_functions\n",
            "[SUCCESS ✅] Instalación inicial completada.\n",
            "\n",
            "[START ▶️] Forzando reinstalación del código del repositorio: https://github.com/DavidP0011/apps_functions\n",
            "[SUCCESS ✅] Reinstalación del código completada.\n",
            "[INFO ℹ️] No se detectó distribución nueva. Se asume 'dpm_apps_functions' para 'apps_functions'.\n",
            "[INFO ℹ️] Para la distribución 'dpm_apps_functions', se detectó el paquete top-level: 'dpm_apps_functions'\n",
            "\n",
            "\n",
            "🔹🔹🔹 Importando el paquete: dpm_apps_functions 🔹🔹🔹\n",
            "\n",
            "[START ▶️] Funciones en el paquete: dpm_apps_functions\n",
            "\n",
            " Módulo: dpm_apps_functions.dpm_apps_utils\n",
            "  - LLM_process_text\n",
            "  - df_to_whisper_transcribe_to_spreadsheet\n",
            "  - files_path_collect_df\n",
            "\n",
            "\n",
            "🔹🔹🔹 [FINISHED ✅] Proceso completado. 🔹🔹🔹\n"
          ]
        }
      ],
      "source": [
        "# @title GIBHUB INSTALL DPM FUNCTIONS\n",
        "config = {\n",
        "    \"github_repo_url_list\": [\n",
        "        # \"https://github.com/DavidP0011/common_functions\",\n",
        "        \"https://github.com/DavidP0011/utils_functions\",\n",
        "        # \"https://github.com/DavidP0011/etl_functions\",\n",
        "        \"https://github.com/DavidP0011/apps_functions\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "\n",
        "install_dpm_repos(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQoX3eAX9SJY",
        "outputId": "a55a7ed8-8d23-4a04-901c-f3fde3c18b00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO ℹ️] Entorno detectado: LOCAL\n",
            "[INFO ℹ️] El entorno 'LOCAL' no requiere montaje de Google Drive.\n"
          ]
        }
      ],
      "source": [
        "# @title IDENTIFICACION DE ENTORNO, INSTALACIÓN GOOGLE DRIVE\n",
        "\n",
        "from dpm_common_functions.dpm_GCP_ini_utils import ini_environment_identification, ini_google_drive_instalation\n",
        "\n",
        "# Detectar el entorno de ejecución\n",
        "ini_environment_identificated = ini_environment_identification()\n",
        "print(f\"[INFO ℹ️] Entorno detectado: {ini_environment_identificated}\", flush=True)\n",
        "\n",
        "GCP_json_keyfile_local = r\"D:\\PYTHON\\proyectos\\api_keys\\animum-dev-apps-venv-python-local.json\"\n",
        "GCP_json_keyfile_colab = \"/content/drive/MyDrive/ANIMUM DIRECCION/DIRECCION BI/NOTEBOOKS/api_keys/animum-dev-datawarehouse-google-colab.json\"\n",
        "GCP_json_keyfile_GCP_secret_id = \"notebook-vm\"\n",
        "\n",
        "# Montar Google Drive si entorno_identificado_str es Colab\n",
        "params = {\"entorno_identificado_str\": ini_environment_identificated}\n",
        "ini_google_drive_instalation(params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-hU3wVkrKz3",
        "outputId": "187a7e74-8a26-4b38-ce22-ebff5e02078e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FFmpeg ya está instalado.\n"
          ]
        }
      ],
      "source": [
        "# @title IMPORTACIÓN DE LIBRERÍAS WHISPER Y FFMPEG\n",
        "\n",
        "# Verificar e instalar el paquete 'whisper'\n",
        "try:\n",
        "    import whisper\n",
        "except ImportError:\n",
        "    print(\"El paquete 'whisper' no está instalado. Procediendo con la instalación...\")\n",
        "    !pip install git+https://github.com/openai/whisper.git\n",
        "    import whisper\n",
        "\n",
        "# Verificar e instalar el paquete 'ffmpeg-python'\n",
        "try:\n",
        "    import ffmpeg\n",
        "except ImportError:\n",
        "    print(\"El paquete 'ffmpeg-python' no está instalado. Procediendo con la instalación...\")\n",
        "    !pip install ffmpeg-python\n",
        "\n",
        "# Instalar y verificar 'ffmpeg' a nivel del sistema\n",
        "import os\n",
        "if os.system(\"ffmpeg -version\") != 0:\n",
        "    print(\"FFmpeg no está instalado. Procediendo con la instalación...\")\n",
        "    !apt-get update\n",
        "    !apt-get install -y ffmpeg\n",
        "    !ffmpeg -version\n",
        "else:\n",
        "    print(\"FFmpeg ya está instalado.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjCHetu3qgUD"
      },
      "source": [
        "# EJECUCIONES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ELsw0CBf_Sn",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# @title RECOPILACIÓN DE RUTAS DE VIDEO\n",
        "\n",
        "# Ejemplo de configuración\n",
        "config = {\n",
        "    \"video_files_root_path\": r\"\\\\animum.local\\Produccion\\LOL-Contenido\\ASIGNATURAS\",\n",
        "    \"video_files_target_search_folder\": [\"CAPITULOS EXPORTAR\",\"CAPITULO EXPORTAR\"],\n",
        "    \"video_files_target_search_extension\": [\".mp4\"],\n",
        "\n",
        "    \"ini_environment_identificated\": ini_environment_identificated,\n",
        "    \"json_keyfile_local\": GCP_json_keyfile_local,\n",
        "    \"json_keyfile_colab\": GCP_json_keyfile_colab,\n",
        "    \"json_keyfile_GCP_secret_id\": GCP_json_keyfile_GCP_secret_id,\n",
        "}\n",
        "\n",
        "# Ejecución de la función con manejo de errores interno\n",
        "from dpm_apps_functions.dpm_apps_utils import files_path_collect_df\n",
        "df_video_files_metadata_scraped = files_path_collect_df(config)\n",
        "df_video_files_metadata_scraped.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "3Y8_IsYhuBfZ",
        "outputId": "a150c1d1-0005-4175-e286-a3d33c222445"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[EXTRACTION [START ⏳]] Extrayendo datos de Google Sheets...\n",
            "[EXTRACTION [SUCCESS ✅]] Datos extraídos con éxito de la hoja 'ARCHIVOS LIST'.\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 6067 entries, 0 to 6066\n",
            "Data columns (total 16 columns):\n",
            " #   Column                   Non-Null Count  Dtype \n",
            "---  ------                   --------------  ----- \n",
            " 0   file_name                6067 non-null   object\n",
            " 1   file_path                6067 non-null   object\n",
            " 2   file_creation_date       6067 non-null   object\n",
            " 3   file_last_modified_date  6067 non-null   object\n",
            " 4   file_scrap_date          6067 non-null   object\n",
            " 5   file_size_mb             6067 non-null   object\n",
            " 6   duration_hms             6067 non-null   object\n",
            " 7   duration_ms              6067 non-null   object\n",
            " 8   video_codec              6067 non-null   object\n",
            " 9   video_bitrate_kbps       6067 non-null   object\n",
            " 10  video_fps                6067 non-null   object\n",
            " 11  video_resolution         6067 non-null   object\n",
            " 12  audio_codec              6067 non-null   object\n",
            " 13  audio_bitrate_kbps       6067 non-null   object\n",
            " 14  audio_channels           6067 non-null   object\n",
            " 15  audio_sample_rate_hz     6067 non-null   object\n",
            "dtypes: object(16)\n",
            "memory usage: 758.5+ KB\n"
          ]
        }
      ],
      "source": [
        "#@title OBTENCIÓN DE INICIAL\n",
        "\n",
        "from dpm_utils_functions.dpm_tables import table_various_sources_to_DF\n",
        "from dpm_common_functions import _ini_authenticate_API\n",
        "\n",
        "params_dic = {\n",
        "    \"spreadsheet_source_table_id\": \"docs.google.com/spreadsheets/d/1SzrRm8nUPPZqU_LE-xdDNdB7rLUdePkjgV4IuPA2U3o\",\n",
        "    \"spreadsheet_source_table_worksheet_name\": \"ARCHIVOS LIST\",\n",
        "\n",
        "    \"ini_environment_identificated\": ini_environment_identificated,\n",
        "    \"json_keyfile_local\": GCP_json_keyfile_local,\n",
        "    \"json_keyfile_colab\": GCP_json_keyfile_colab,\n",
        "    \"json_keyfile_GCP_secret_id\": GCP_json_keyfile_GCP_secret_id,\n",
        "}\n",
        "\n",
        "df_video_files_metadata_initial = table_various_sources_to_DF(params_dic)\n",
        "df_video_files_metadata_initial.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sN1RwECauBfZ",
        "outputId": "e36fbb49-45d6-4d53-ee59-ab335babeb16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔹🔹🔹 [START ▶️] DTYPE COPY\n",
            "[DTYPE COPY ℹ️] (1/16) 'file_name' ya es object. Skipped.\n",
            "[DTYPE COPY ℹ️] (2/16) 'file_path' ya es object. Skipped.\n",
            "[DTYPE COPY ✅] (3/16) 'file_creation_date' → datetime64[ns].\n",
            "[DTYPE COPY ✅] (4/16) 'file_last_modified_date' → datetime64[ns].\n",
            "[DTYPE COPY ✅] (5/16) 'file_scrap_date' → datetime64[ns].\n",
            "[DTYPE COPY ✅] (6/16) 'file_size_mb' → int64.\n",
            "[DTYPE COPY ℹ️] (7/16) 'duration_hms' ya es object. Skipped.\n",
            "[DTYPE COPY ✅] (8/16) 'duration_ms' → Int64.\n",
            "[DTYPE COPY ℹ️] (9/16) 'video_codec' ya es object. Skipped.\n",
            "[DTYPE COPY ✅] (10/16) 'video_bitrate_kbps' → int64.\n",
            "[DTYPE COPY ✅] (11/16) 'video_fps' → float64.\n",
            "[DTYPE COPY ℹ️] (12/16) 'video_resolution' ya es object. Skipped.\n",
            "[DTYPE COPY ℹ️] (13/16) 'audio_codec' ya es object. Skipped.\n",
            "[DTYPE COPY ✅] (14/16) 'audio_bitrate_kbps' → int64.\n",
            "[DTYPE COPY ✅] (15/16) 'audio_channels' → int64.\n",
            "[DTYPE COPY ✅] (16/16) 'audio_sample_rate_hz' → int64.\n",
            "[DTYPE COPY ✔️] Cast fin — ok: 10/16 | fail: 0 | skipped: 6\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 6067 entries, 0 to 6066\n",
            "Data columns (total 16 columns):\n",
            " #   Column                   Non-Null Count  Dtype         \n",
            "---  ------                   --------------  -----         \n",
            " 0   file_name                6067 non-null   object        \n",
            " 1   file_path                6067 non-null   object        \n",
            " 2   file_creation_date       6067 non-null   datetime64[ns]\n",
            " 3   file_last_modified_date  6067 non-null   datetime64[ns]\n",
            " 4   file_scrap_date          6067 non-null   datetime64[ns]\n",
            " 5   file_size_mb             6067 non-null   int64         \n",
            " 6   duration_hms             6067 non-null   object        \n",
            " 7   duration_ms              6066 non-null   Int64         \n",
            " 8   video_codec              6067 non-null   object        \n",
            " 9   video_bitrate_kbps       6067 non-null   int64         \n",
            " 10  video_fps                6067 non-null   float64       \n",
            " 11  video_resolution         6067 non-null   object        \n",
            " 12  audio_codec              6067 non-null   object        \n",
            " 13  audio_bitrate_kbps       6067 non-null   int64         \n",
            " 14  audio_channels           6067 non-null   int64         \n",
            " 15  audio_sample_rate_hz     6067 non-null   int64         \n",
            "dtypes: Int64(1), datetime64[ns](3), float64(1), int64(5), object(6)\n",
            "memory usage: 764.4+ KB\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 212 entries, 0 to 211\n",
            "Data columns (total 16 columns):\n",
            " #   Column                   Non-Null Count  Dtype         \n",
            "---  ------                   --------------  -----         \n",
            " 0   file_name                212 non-null    object        \n",
            " 1   file_path                212 non-null    object        \n",
            " 2   file_creation_date       212 non-null    datetime64[ns]\n",
            " 3   file_last_modified_date  212 non-null    datetime64[ns]\n",
            " 4   file_scrap_date          212 non-null    datetime64[ns]\n",
            " 5   file_size_mb             212 non-null    int64         \n",
            " 6   duration_hms             212 non-null    object        \n",
            " 7   duration_ms              212 non-null    int64         \n",
            " 8   video_codec              212 non-null    object        \n",
            " 9   video_bitrate_kbps       212 non-null    int64         \n",
            " 10  video_fps                212 non-null    float64       \n",
            " 11  video_resolution         212 non-null    object        \n",
            " 12  audio_codec              212 non-null    object        \n",
            " 13  audio_bitrate_kbps       212 non-null    int64         \n",
            " 14  audio_channels           212 non-null    int64         \n",
            " 15  audio_sample_rate_hz     212 non-null    int64         \n",
            "dtypes: datetime64[ns](3), float64(1), int64(6), object(6)\n",
            "memory usage: 26.6+ KB\n"
          ]
        }
      ],
      "source": [
        "#@title APLICACIÓN DE DTYPES\n",
        "config = {\n",
        "    \"reference_dtype_df\": df_video_files_metadata_scraped,   # DataFrame con los dtypes de referencia\n",
        "    \"target_dtype_df\": df_video_files_metadata_initial    # DataFrame al que se le aplicarán esos dtypes\n",
        "}\n",
        "from dpm_utils_functions.dpm_tables import DType_df_to_df\n",
        "df_video_files_metadata_initial_casted, meta = DType_df_to_df(config)\n",
        "df_video_files_metadata_initial_casted.info()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "dFAeZbfAuBfZ",
        "outputId": "c7745758-c1eb-405e-c443-9808455fe0b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CONSOLIDATION START ▶️] 2025-06-12T15:46:46\n",
            "INFO ℹ️ id_fields=['file_name', 'duration_ms'] | policy=keep_newest\n",
            "FINISHED ✅ registros finales=6064 | duplicados_resueltos=215\n"
          ]
        }
      ],
      "source": [
        "#@title CONSOLIDACIÓN DE DF\n",
        "\n",
        "config = {\n",
        "    \"validate_df_schemas_match\" : True, # valida esquema primero\n",
        "\n",
        "    \"df_initial\"   : df_video_files_metadata_initial_casted,\n",
        "    \"df_to_merge\"  : df_video_files_metadata_scraped,\n",
        "    \"id_fields\"    : [\"file_name\", \"duration_ms\"],\n",
        "\n",
        "    \"duplicate_policy\": \"keep_newest\",   # keep_newest | keep_oldest | keep_df_initial | keep_df_to_merge\n",
        "    \"duplicate_date_field\"       : \"file_scrap_date\",\n",
        "    \"duplicate_date_field_format_str\"  : \"%d/%m/%Y\",\n",
        "\n",
        "\n",
        "    \"return_metadata\"  : True\n",
        "}\n",
        "\n",
        "\n",
        "# Llamada a la función\n",
        "# from dpm_utils_functions.dpm_tables import tables_consolidate_duplicates_df\n",
        "df_video_files_metadata_consolidated, metadata = tables_consolidate_duplicates_df(config)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRN-RQYWuBfa",
        "outputId": "7a2f8a70-db51-4456-9ded-6c50b87bebce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🔹🔹🔹 [START ▶️] Iniciando escritura de DataFrame en destino configurado 🔹🔹🔹\n",
            "\n",
            "[METRICS [INFO ℹ️]] DataFrame recibido: 6064 filas × 16 columnas.\n",
            "\n",
            "[LOAD [START ▶️]] Iniciando escritura en Google Sheets…\n",
            "[LOAD [SUCCESS ✅]] Se actualizaron N/A celdas en Google Sheets.\n",
            "[METRICS [INFO ℹ️]] Destino final: https://docs.google.com/spreadsheets/d/1SzrRm8nUPPZqU_LE-xdDNdB7rLUdePkjgV4IuPA2U3o\n",
            "\n",
            "🔹🔹🔹 [END [FINISHED ✅]] Escritura completada exitosamente. 🔹🔹🔹\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# @title SUBIDA A GOOGLE SHEETS\n",
        "config = {\n",
        "    \"df\": df_video_files_metadata_consolidated,\n",
        "    'spreadsheet_target_table_id': 'https://docs.google.com/spreadsheets/d/1SzrRm8nUPPZqU_LE-xdDNdB7rLUdePkjgV4IuPA2U3o',\n",
        "    # Nombre de la pestaña (worksheet) destino\n",
        "    'spreadsheet_target_table_worksheet_name': 'ResponseAPI',\n",
        "\n",
        "    \"ini_environment_identificated\": ini_environment_identificated,\n",
        "    \"json_keyfile_local\": GCP_json_keyfile_local,\n",
        "    \"json_keyfile_colab\": GCP_json_keyfile_colab,\n",
        "    \"json_keyfile_GCP_secret_id\": GCP_json_keyfile_GCP_secret_id,\n",
        "}\n",
        "from dpm_utils_functions.dpm_tables import table_DF_to_various_targets\n",
        "\n",
        "table_DF_to_various_targets(config)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}